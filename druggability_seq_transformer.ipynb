{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QToUCvADa4l7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "\n",
        "!pip install biopython\n",
        "import Bio\n",
        "from Bio import SeqIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fastas(path):\n",
        "  \"\"\"\n",
        "  Extracts the sequences and IDs of proteins from a FASTA file.\n",
        "\n",
        "  Args:\n",
        "    path (str): Filepath of the FASTA file\n",
        "  \n",
        "  Returns:\n",
        "    df (DataFrame): A Pandas dataframe containing the protein IDs and sequences\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  ids = []\n",
        "  seqs = []\n",
        "\n",
        "  with open(path) as handle:\n",
        "      for record in Bio.SeqIO.parse(handle, \"fasta\"):\n",
        "          ids.append(record.id)\n",
        "          seqs.append(str(record.seq))\n",
        "          \n",
        "  df = pd.DataFrame(columns=['id', 'seq'])\n",
        "  df['id'] = ids\n",
        "  df['seq'] = seqs\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "d-dlnli8a9o-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the positive and negative proteins in the dataset\n",
        "\n",
        "pos_train = get_fastas(\"data/positive_train_1.txt\")\n",
        "neg_train = get_fastas(\"data/negative_train_1.txt\")\n",
        "\n",
        "pos_test = get_fastas(\"data/positive_train_2.txt\")\n",
        "neg_test = get_fastas(\"data/negative_train_2.txt\")\n",
        "\n",
        "pos_train['label'] = 1\n",
        "neg_train['label'] = 0\n",
        "\n",
        "pos_test['label'] = 1\n",
        "neg_test['label'] = 0\n",
        "\n",
        "full = pd.concat([pos_train, neg_train, pos_test, neg_test])\n",
        "\n",
        "full = full.sample(frac=1)\n",
        "\n",
        "full"
      ],
      "metadata": {
        "id": "Ecm66Cwya_WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import EsmTokenizer, TFEsmForSequenceClassification"
      ],
      "metadata": {
        "id": "_kMwC-Mjf9Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses the ESM tokenizer to encode the protein sequences\n",
        "\n",
        "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
        "\n",
        "encodings = tokenizer(list(full['seq']), return_tensors='tf', truncation=True, padding=True)['input_ids']\n",
        "\n",
        "encodings"
      ],
      "metadata": {
        "id": "Hr9ML6SNhCNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(train_encodings, train_labels, test_encodings, test_labels, learning_rate=1e-4, batch_size=16):\n",
        "  \"\"\"\n",
        "  Fine-tunes a pretrained ESM sequence classsifier model on the training set and\n",
        "  returns the model.\n",
        "\n",
        "  Args:\n",
        "    train_encodings (tf.tensor): Tokenized encodings of the training samples\n",
        "    train_labels (np.array): Labels of the training samples\n",
        "    test_encodings (tf.tensor): Tokenized encodings of the test samples\n",
        "    test_labels (np.array): Labels of the test samples\n",
        "    learning_rate (float): Learning rate for training (default=1e-4)\n",
        "    batch_size (float): Batch size for training (default=16)\n",
        "\n",
        "  Returns:\n",
        "    model (tf.model): Trained model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Imports a pre-trained ESM-2 model with classification head\n",
        "\n",
        "  model = TFEsmForSequenceClassification.from_pretrained(\n",
        "      \"facebook/esm2_t6_8M_UR50D\", \n",
        "      num_labels=1)\n",
        "  \n",
        "\n",
        "  # Stops training if test loss has not improved for 2 epochs\n",
        "\n",
        "  callbacks = [tf.keras.callbacks.EarlyStopping(patience=2, \n",
        "                                                restore_best_weights=True)]\n",
        "\n",
        "\n",
        "  # beta_2 setting was taken from the original ESM-2 publication\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_2=0.98)\n",
        "\n",
        "  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  with tf.device('/GPU:0'):\n",
        "    model.fit(train_encodings, \n",
        "              train_labels, \n",
        "              epochs=15, \n",
        "              batch_size=batch_size,\n",
        "              verbose=1, \n",
        "              validation_data=[test_encodings, test_labels], \n",
        "              callbacks=callbacks)\n",
        "    \n",
        "  return model"
      ],
      "metadata": {
        "id": "eK1nTqvWaQMr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# hp_values is a list of hyperparameter settings to try\n",
        "\n",
        "hp_values = [4, 8, 16, 32]\n",
        "n_hp_values = len(hp_values)\n",
        "\n",
        "\n",
        "# Splits the dataset into n_splits sets for cross-validation\n",
        "\n",
        "n_splits = 5\n",
        "splits = KFold(n_splits=n_splits)\n",
        "\n",
        "\n",
        "# Initializes a NumPy array to hold the accuracy and AUC values for each\n",
        "# hyperparameter setting\n",
        "\n",
        "acc_results = np.zeros((n_splits, n_hp_values))\n",
        "auc_results = np.zeros((n_splits, n_hp_values))\n",
        "\n",
        "\n",
        "# Iterates through each split and tests each hyperparameter value on that split\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(splits.split(full['seq'])):\n",
        "\n",
        "  # Splits the encodings and labels\n",
        "\n",
        "  train_encodings = tf.gather(encodings, indices=train_index)\n",
        "  test_encodings = tf.gather(encodings, indices=test_index)\n",
        "\n",
        "  train_labels = np.array(full['label'].iloc[train_index])\n",
        "  test_labels = np.array(full['label'].iloc[test_index])\n",
        "\n",
        "\n",
        "  # Iterates through the hyperparameter values to be tested and fits a model\n",
        "  # using each one\n",
        "\n",
        "  for j, value in enumerate(hp_values):\n",
        "    print('Split #: ', i + 1, ' Parameter val: ', value)\n",
        "\n",
        "\n",
        "    # This configuration was used to modulate the hyperparameter value\n",
        "    # batch_size; it can be changed to modulate learning_rate or any other model\n",
        "    # hyperparameter by adding them to the fit_model args\n",
        "\n",
        "    model = fit_model(train_encodings, \n",
        "                      train_labels, \n",
        "                      test_encodings, \n",
        "                      test_labels, \n",
        "                      batch_size=value)\n",
        "    \n",
        "\n",
        "    # Generates predictions for the test set using the trained model and tests\n",
        "    # their accuracy and AUC, saving them to the \"_results\" arrays\n",
        "\n",
        "    logits = model.predict(test_encodings, batch_size=8)\n",
        "\n",
        "    probs = tf.keras.activations.sigmoid(logits.logits)\n",
        "\n",
        "    acc_results[i][j] = accuracy_score(test_labels, np.round(probs))\n",
        "    auc_results[i][j] = roc_auc_score(test_labels, probs)"
      ],
      "metadata": {
        "id": "osid4s7qgCXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saves and downloads the accuracy and AUC results as a single zip file. This\n",
        "# configuration is designed for use with Google Colab\n",
        "\n",
        "acc_results = pd.DataFrame(acc_results, columns=hp_values)\n",
        "auc_results = pd.DataFrame(auc_results, columns=hp_values)\n",
        "\n",
        "acc_results.to_csv('accuracy_results.csv')\n",
        "auc_results.to_csv('auc_results.csv')\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "\n",
        "zip = ZipFile('zipped_results.zip', 'w')\n",
        "\n",
        "zip.write('accuracy_results.csv')\n",
        "zip.write('auc_results.csv')\n",
        "\n",
        "zip.close()\n",
        "\n",
        "files.download(\"zipped_results.zip\")"
      ],
      "metadata": {
        "id": "IMDuTxujIC4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the independent evaluation set\n",
        "\n",
        "pos_evaluation = get_fastas(\"data/positive_test.txt\")\n",
        "neg_evaluation = get_fastas(\"data/negative_test.txt\")\n",
        "\n",
        "pos_evaluation['label'] = 1\n",
        "neg_evaluation['label'] = 0\n",
        "\n",
        "evaluation = pd.concat([pos_evaluation, neg_evaluation])\n",
        "\n",
        "evaluation"
      ],
      "metadata": {
        "id": "E-fvffLZY6mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates the tokenized encodings for the evaluation set\n",
        "\n",
        "eval_encodings = tokenizer(list(evaluation['seq']), return_tensors='tf', truncation=True, padding=True)['input_ids']"
      ],
      "metadata": {
        "id": "X66Gx8OAaXf6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Trains a model on the full training set and gets its performance for the \n",
        "# evaluation set\n",
        "\n",
        "train_labels = np.array(full['label'])\n",
        "eval_labels = np.array(evaluation['label'])\n",
        "\n",
        "model = fit_model(encodings, train_labels, eval_encodings, eval_labels)\n",
        "\n",
        "logits = model.predict(eval_encodings, batch_size=8)\n",
        "\n",
        "probs = tf.keras.activations.sigmoid(logits.logits)\n",
        "\n",
        "acc = accuracy_score(eval_labels, np.round(probs))\n",
        "auc = roc_auc_score(eval_labels, probs)"
      ],
      "metadata": {
        "id": "AOaBcV6habAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}